from transformers import XLMRobertaTokenizer
import os
import torch
from torchvision import transforms
from timm.data.constants import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
import utils

from pathlib import Path
from infer_utils import create_dataset_by_split,read_image_path_from_jsonl,load_config

from timm.models import create_model
from engine_for_finetuning import get_handler

from PIL import Image
from modeling_utils import _get_base_config, _get_large_config
from modeling_finetune import BEiT3ForRetrieval
import torch
device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
class BEIT3Tokenizer(BEiT3ForRetrieval):
    def __init__(self, args, tokenizer=None, model=None):
        # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„é…ç½®ç”Ÿæˆæ–¹å¼
        if not hasattr(args, 'model'):
            raise ValueError("args must specify 'model' name")
            
        # æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©é…ç½®
        if 'large' in args.model.lower():
            model_config = _get_large_config(img_size=384,**vars(args))
        else:
            model_config = _get_base_config(img_size=384,**vars(args))
            
        # åˆå¹¶ç”¨æˆ·å‚æ•°å’Œæ¨¡å‹é»˜è®¤é…ç½®
        for k, v in vars(model_config).items():
            if not hasattr(args, k):
                setattr(args, k, v)
                
        super().__init__(args)  # ç°åœ¨argsåŒ…å«æ‰€æœ‰å¿…éœ€å‚æ•°
        self.args = args
        self.tokenizer = tokenizer
        self.bos_token_id = tokenizer.bos_token_id
        self.eos_token_id = tokenizer.eos_token_id
        self.pad_token_id = tokenizer.pad_token_id
        # åˆå§‹åŒ–BEiT3æ¨¡å‹
        self.model = create_model(
            args.model,
            pretrained=False,
            drop_path_rate=args.drop_path,
            vocab_size=args.vocab_size,
            checkpoint_activations=args.checkpoint_activations,
        ).to(args.device)
    

    def tokenizer_Input_Data(self, text, tokenizer, max_len=None):
    # ä½¿ç”¨tokenizerçš„__call__æ–¹æ³•ï¼ˆæ¨èæ–¹å¼ï¼‰
        if isinstance(text, str):
            tokens = tokenizer.tokenize(text)
        else:
            tokens = text[:]
        tokens = tokenizer.convert_tokens_to_ids(tokens)
        if len(tokens) == 0:
            raise RuntimeError("The text segment should contains at least one tokens!")
        if max_len is None:
            max_len = args.num_max_bpe_tokens

        if len(tokens) > max_len - 2:
            tokens = tokens[:max_len - 2]

        tokens = [self.bos_token_id] + tokens[:] + [self.eos_token_id]
        num_tokens = len(tokens)
        padding_mask = [0] * num_tokens + [1] * (max_len - num_tokens)

        tokens_tensor = torch.tensor(tokens + [self.pad_token_id] * (max_len - num_tokens), dtype=torch.long).unsqueeze(0).to(device)  # [1, seq_len]
        padding_mask_tensor = torch.tensor(padding_mask, dtype=torch.long).unsqueeze(0).to(device)

        return tokens_tensor, padding_mask_tensor, num_tokens
    
    
    def tokenizer_Input_Image(self,image_path):
        image = Image.open(image_path).convert("RGB")
        transform = transforms.Compose([
            transforms.Resize((args.input_size, args.input_size), interpolation=3),
            transforms.ToTensor(),
            transforms.Normalize(mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD)
        ])
        image_token = transform(image).unsqueeze(0).to(self.args.device)
        # outputs = self.beit3(
        #         textual_tokens=None, 
        #         visual_tokens=image_token, 
        #         text_padding_position=None, 
        #     )
        # x = outputs["encoder_out"]
        # vision_cls = self.vision_head(x[:, 0, :])
        # vision_cls = F.normalize(vision_cls, dim=-1)
        return image_token

    def infer_model(self, model, image, language_tokens, padding_mask):
        print("TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT")
        print(type(model))
        with torch.no_grad():
            vision_cls, _ = model(image=image, only_infer=True)
            _, language_cls = model(
            text_description=language_tokens, padding_mask=padding_mask, only_infer=True)
        print(f"Batch Summary:")
        print(f"  Image features shape: {vision_cls.shape}")
        print(f"  Text features shape: {language_cls.shape}")
        return vision_cls, language_cls
    
# åˆ¶ä½œtokenåŒ–
tokenizer = XLMRobertaTokenizer("./model/beit3.spm")
# è¯»å–é…ç½®è·¯å¾„
config_path = "./inference_config.json"
# è¯»å‚æ•°
args = load_config(config_path)
# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
if hasattr(args, 'output_dir') and args.output_dir:
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)


device = torch.device(args.device)
beit3_model = BEIT3Tokenizer(
    args=args,
    tokenizer=tokenizer
)

# ç”ŸæˆCOCOçš„ç´¢å¼•æ–‡ä»¶ï¼ˆä¼šç”Ÿæˆä¸Šè¿°.jsonlæ–‡ä»¶ï¼‰
language_tokens,padding_mask,num_tokens = beit3_model.tokenizer_Input_Data("A crocodile",tokenizer)
print(f"token_id{language_tokens}padding_mask{padding_mask}")
data_path = "./data/aac"
image_paths = read_image_path_from_jsonl(data_path)
# image_tensors = beit3_model.tokenizer_Input_Image("/home/user1/liuduanye/unilm/beit3/data/aac/val/zebra.png").unsqueeze(0).to(device)


image_tensors = []
image_path_list = [] 
for item in image_paths:  # item æ˜¯å­—å…¸
    full_img_path = os.path.join(data_path, item["image_path"])  # ä»å­—å…¸ä¸­æå–è·¯å¾„
    img_tensor = beit3_model.tokenizer_Input_Image(full_img_path)
    image_tensors.append(img_tensor)
    image_path_list.append(full_img_path)

# ç°åœ¨ image_tensors æ˜¯æ‰€æœ‰å›¾åƒå¤„ç†åçš„å¼ é‡åˆ—è¡¨
print(f"å…±å¤„ç† {len(image_tensors)} å¼ å›¾åƒ")
print("ç¤ºä¾‹å¼ é‡å½¢çŠ¶:", image_tensors[0].shape)

device = torch.device("cuda:1")


if args.finetune:
    print(f"ğŸš€ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡: {args.finetune}")
    utils.load_model_and_may_interpolate(
    args.finetune,
    beit3_model,
    args.model_key,          # é»˜è®¤æ˜¯ 'model|module'
    args.model_prefix,    # é»˜è®¤æ˜¯ ''
)
print(f"âœ… æƒé‡åŠ è½½æˆåŠŸ! (æ¥æº: {args.finetune})")
beit3_model.to(device)

task_handler = get_handler(args)
data_loader = create_dataset_by_split(args,"test")
task_handler.before_infer()
torch.cuda.empty_cache()
with torch.no_grad():
    with torch.cuda.amp.autocast():
        for img_tensor, img_path in zip(image_tensors, image_path_list):  # åŒæ—¶è¿­ä»£å¼ é‡å’Œè·¯å¾„
            task_handler.infer_batch(
                model=beit3_model,
                image=img_tensor.unsqueeze(0) if len(img_tensor.shape) == 3 else img_tensor,  # ç¡®ä¿æœ‰batchç»´åº¦
                language_tokens=language_tokens,
                padding_mask=padding_mask,
                image_path=img_path  # ä¼ å…¥å½“å‰å›¾åƒè·¯å¾„
            )

results = task_handler.after_infer()


print("\n===== ç‰¹å¾å’Œç›¸ä¼¼åº¦ç»Ÿè®¡ =====")
print(f"å›¾åƒç‰¹å¾å½¢çŠ¶: {results['image_features'].shape} (åº”å¦‚ [N, D])")
print(f"æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {results['text_features'].shape} (åº”å¦‚ [1, D])")
print(f"ç›¸ä¼¼åº¦çŸ©é˜µå½¢çŠ¶: {results['similarity_scores'].shape} (åº”å¦‚ [N, 1])")

# è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
scores = results['similarity_scores'].squeeze()  # ä»[N,1]å˜ä¸º[N]
print(f"\n===== ç›¸ä¼¼åº¦ç»Ÿè®¡ =====")
print(f"æœ€å¤§å€¼: {scores.max().item():.4f}")
print(f"æœ€å°å€¼: {scores.min().item():.4f}")
print(f"å¹³å‡å€¼: {scores.mean().item():.4f}")
print(f"æ ‡å‡†å·®: {scores.std().item():.4f}")

# Top-Kåˆ†æï¼ˆå¸¦è·¯å¾„è¾“å‡ºï¼‰
top_values, top_indices = torch.topk(scores, k=5)
# ä¿®æ”¹Top-Kæ‰“å°éƒ¨åˆ†ï¼š
print("\nTop-5ç›¸ä¼¼åº¦:")
for rank, (val, idx) in enumerate(zip(top_values.tolist(), top_indices.tolist())):
    # ä½¿ç”¨å­˜å‚¨çš„è·¯å¾„åˆ—è¡¨
    img_path = results['image_paths'][idx] if 'image_paths' in results else f"æœªçŸ¥è·¯å¾„ï¼ˆç´¢å¼•{idx})"
    print(f"{rank+1}. è·¯å¾„={img_path}, åˆ†æ•°={val:.4f}")

# ç‰¹å¾å¤šæ ·æ€§åˆ†æ
img_feats = results['image_features']
diff_matrix = torch.cdist(img_feats, img_feats, p=2)
mean_diff = diff_matrix.mean().item()
print(f"\nå›¾åƒç‰¹å¾å¹³å‡å·®å¼‚: {mean_diff:.4f} (ç†æƒ³å€¼>0.3)")

# æ£€æŸ¥å¼‚å¸¸å€¼ï¼ˆå¸¦è·¯å¾„è¾“å‡ºï¼‰
abnormal_indices = torch.where(scores < -0.5)[0]
if len(abnormal_indices) > 0:
    print(f"\nè­¦å‘Š: å‘ç°{len(abnormal_indices)}ä¸ªå¼‚å¸¸ä½ç›¸ä¼¼åº¦å€¼(< -0.5)")
    for idx in abnormal_indices:
        img_path = image_paths[idx] if hasattr(task_handler, 'image_paths') else f"æœªçŸ¥è·¯å¾„ï¼ˆç´¢å¼•{idx}ï¼‰"
        print(f"å¼‚å¸¸ç´¢å¼•={idx}, è·¯å¾„={img_path}, åˆ†æ•°={scores[idx]:.4f}")
