import torch
import argparse
import os
from pathlib import Path
import json
import numpy as np
import time
import torch.backends.cudnn as cudnn

from timm.models import create_model
from engine_for_finetuning import get_handler, evaluate
from datasets import create_downstream_dataset
import utils
import modeling_finetune
# å°±æ˜¯æŠŠæ•°æ®é›†æ”¹æˆçš„å•ä¸ªï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ªçº¯é¢„æµ‹æ¨¡æ¿
def load_config(json_path):
    """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®å¹¶è½¬æ¢ä¸ºNamespaceå¯¹è±¡"""
    try:
        with open(json_path, 'r') as f:
            config = json.load(f)
        
        # éªŒè¯å¿…è¦å­—æ®µ
        required_fields = ['model', 'task', 'finetune']
        for field in required_fields:
            if field not in config:
                raise ValueError(f"é…ç½®ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
        
        # è·¯å¾„å¤„ç†
        path_fields = ['finetune', 'output_dir', 'sentencepiece_model', 'data_path']
        for field in path_fields:
            if field in config and config[field]:
                config[field] = str(Path(config[field]).resolve())
        
        return argparse.Namespace(**config)
    
    except FileNotFoundError:
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
    except json.JSONDecodeError:
        raise ValueError(f"é…ç½®æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {json_path}")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½é…ç½®å¤±è´¥: {str(e)}")

def main(args):
    device = torch.device(args.device)

    # åˆå§‹åŒ–æ¨¡å‹
    model = create_model(
        args.model,
        pretrained=False,
        drop_path_rate=args.drop_path,
        vocab_size=args.vocab_size,
        checkpoint_activations=args.checkpoint_activations,
    )
    print(type(model))
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if args.finetune:
        print(f"ğŸš€ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡: {args.finetune}")
        utils.load_model_and_may_interpolate(
        args.finetune,
        model,
        args.model_key,          # é»˜è®¤æ˜¯ 'model|module'
        args.model_prefix,    # é»˜è®¤æ˜¯ ''
    )
        print(f"âœ… æƒé‡åŠ è½½æˆåŠŸ! (æ¥æº: {args.finetune})")

    model.to(device)
    model.eval()

    # è·å–ä»»åŠ¡å¤„ç†å™¨ï¼ˆå¤„ç†è¾“å…¥è¾“å‡ºï¼‰
    task_handler = get_handler(args)

    # åŠ è½½æ•°æ®é›†ï¼ˆä»…æ¨ç†æ•°æ®ï¼‰
    data_loader = create_downstream_dataset(args, is_eval=True)

    # æ‰§è¡Œæ¨ç†
    if args.task in ["nlvr2", "flickr30k", "coco_retrieval", "imagenet"]:
        # åˆ†ç±»/æ£€ç´¢ä»»åŠ¡
        test_stats, task_key = evaluate(data_loader, model, device, task_handler)
        print(type(model))
        print(f"æ¨¡å‹åœ¨ {len(data_loader.dataset)} æµ‹è¯•æ ·æœ¬ä¸Šçš„æŒ‡æ ‡ [{task_key}]: {test_stats[task_key]:.3f}%")

if __name__ == "__main__":
    # ä»JSONæ–‡ä»¶åŠ è½½é…ç½®
    config_path = "./inference_config.json"
    args = load_config(config_path)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if hasattr(args, 'output_dir') and args.output_dir:
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    main(args)